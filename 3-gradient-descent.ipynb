{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Traverses the contour plot to find the path of steepest descent and converge on the minimum. This is extremely useful for mathematically computing the minimum of the cost function without needing to plot and visually select the optimal parameters. The starting point isn't resistant to local minima, and can improperly give poor results for the global minimum.\n",
    "\n",
    "The gradient descent outlined here is also called \"Batch Gradient Descent\" since it uses all the training examples (the _Sum (of all m)_ of Squared Error).\n",
    "\n",
    "General steps:\n",
    "\n",
    "1. Start with initial guess\n",
    "2. compute the cost\n",
    "3. comppute the direction of the next step (towards a local minimum)\n",
    "4. iterate the guess towards the minimum\n",
    "5. repeat 2-4 until the cost converges (below a threshold)\n",
    "\n",
    "## Terminology\n",
    "\n",
    "Update both of these functions at the _same time_ (use same original variables):\n",
    "\n",
    "$w_{n+1} = w_{n}-\\alpha\\frac{\\delta J(w_{n}, b_{n})}{\\delta w_{n}}$\n",
    "\n",
    "$b_{n+1} = b_{n}-\\alpha\\frac{\\delta J(w_{n}, b_{n})}{\\delta b_{n}}$\n",
    "\n",
    "1. $w_{n}$: current guess for weight parameter\n",
    "2. $b_{n}$: current guess for bias parameter\n",
    "3. $w_{n+1}$: next guess for weight parameter\n",
    "4. $b_{n+1}$: next guess for bias parameter\n",
    "5. $\\alpha$: the learning rate, how big the step is\n",
    "6. $\\frac{\\delta J(w, b)}{\\delta w}$ PD of the cost function wrt weight parameter, provides the direction wrt weight\n",
    "7. $\\frac{\\delta J(w, b)}{\\delta b}$ PD of the cost function wrt bias parameter, provides the direction wrt bias\n",
    "\n",
    "Defining the derivative terms:\n",
    "\n",
    "$\\frac{\\delta J(w, b)}{\\delta w} = \\frac{1}{m}\\sum_{i=0}^{m-1}(f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)}$\n",
    "\n",
    "$\\frac{\\delta J(w, b)}{\\delta b} = \\frac{1}{m}\\sum_{i=0}^{m-1}(f_{w,b}(x^{(i)}) - y^{(i)})$\n",
    "\n",
    "## Selecting the Learning Rate\n",
    "\n",
    "Has a huge impact on the efficiency of the model.\n",
    "\n",
    "- < $\\alpha$ causes GD to be slow, but it will converge\n",
    "- \\> $\\alpha$ causes GD to overshoot & may never converge (or even diverge)\n",
    "- as you approach a local minima, the derivative will be less and less, resulting in a smaller step for each iteration\n",
    "\n",
    "## Applying to Linear Regression\n",
    "\n",
    "![ ](gd-regression.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_gradient(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      x (ndarray (m,)): Data, m examples \n",
    "      y (ndarray (m,)): target values\n",
    "      w,b (scalar)    : model parameters  \n",
    "    Returns\n",
    "      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w\n",
    "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b     \n",
    "     \"\"\"\n",
    "    \n",
    "    # Number of training examples\n",
    "    m = x.shape[0]    \n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    \n",
    "    for i in range(m):  \n",
    "        f_wb = w * x[i] + b \n",
    "        dj_dw_i = (f_wb - y[i]) * x[i] \n",
    "        dj_db_i = f_wb - y[i] \n",
    "        dj_db += dj_db_i\n",
    "        dj_dw += dj_dw_i \n",
    "    dj_dw = dj_dw / m \n",
    "    dj_db = dj_db / m \n",
    "        \n",
    "    return dj_dw, dj_db\n",
    "\n",
    "\n",
    "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function): \n",
    "    \"\"\"\n",
    "    Performs gradient descent to fit w,b. Updates w,b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,))  : Data, m examples \n",
    "      y (ndarray (m,))  : target values\n",
    "      w_in,b_in (scalar): initial values of model parameters  \n",
    "      alpha (float):     Learning rate\n",
    "      num_iters (int):   number of iterations to run gradient descent\n",
    "      cost_function:     function to call to produce cost\n",
    "      gradient_function: function to call to produce gradient\n",
    "      \n",
    "    Returns:\n",
    "      w (scalar): Updated value of parameter after running gradient descent\n",
    "      b (scalar): Updated value of parameter after running gradient descent\n",
    "      J_history (List): History of cost values\n",
    "      p_history (list): History of parameters [w,b] \n",
    "      \"\"\"\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    b = b_in\n",
    "    w = w_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters using gradient_function\n",
    "        dj_dw, dj_db = gradient_function(x, y, w , b)     \n",
    "\n",
    "        # Update Parameters using equation (3) above\n",
    "        b = b - alpha * dj_db                            \n",
    "        w = w - alpha * dj_dw                            \n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( cost_function(x, y, w , b))\n",
    "            p_history.append([w,b])\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e} \",\n",
    "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    " \n",
    "    return w, b, J_history, p_history #return w and J,w history for graphing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
