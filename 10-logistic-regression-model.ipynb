{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model\n",
    "\n",
    "## Cost Function\n",
    "\n",
    "SSE is not the ideal cost function here, since the cost is non-convex thrwarted by many local minima.\n",
    "\n",
    "![ ](bad-cost.png)\n",
    "\n",
    "Calling the loss on a single training example:\n",
    "\n",
    "$loss = L(f_{\\vec{w},b}(\\vec{x}^{(i)}), y^{(i)})$\n",
    "\n",
    "Collecting the loss functions into a parent piecewise function is done to force into convex submission. Rearranging the SSE cost function:\n",
    "\n",
    "$J(w, \\vec{b}) = \\frac{1}{m}\\sum_{i=1}^m(L(f_{\\vec{w}, b}(x^{(i)}) - y^{(i)}))$\n",
    "\n",
    "Where $L$ is defined as (yes, it removes the $\\frac{1}{2}$ from the linear regression model):\n",
    "\n",
    "$L(f_{\\vec{w},b}(\\vec{x}^{(i)}), y^{(i)}) = \n",
    "\\begin{cases}\n",
    " -log(f_{\\vec{w},b}(\\vec{x}^{(i)}))   &\\text{if } y^{(i)} = 1\\\\\n",
    " -log(1 - f_{\\vec{w},b}(\\vec{x}^{(i)}))   &\\text{if } y^{(i)} = 0\n",
    "\\end{cases}$\n",
    "\n",
    "Since $f$ is the output of logistic regression, it will always be on the range of 0 to 1. Plotting the relevant part of the curve for $y^{(i)} = 1$ with the x & y axis as the prediction function $f$ and loss $L$ respectively:\n",
    "\n",
    "![ ](neg-log.png)\n",
    "\n",
    "As $f_{\\vec{w},b}(\\vec{x}^{(i)}) -> 1$ then $loss -> 0$  \n",
    "As $f_{\\vec{w},b}(\\vec{x}^{(i)}) -> 0$ then $loss -> inf$\n",
    "\n",
    "Plotting the relevant part of the curve for $y^{(i)} = 1$ with the x & y axis as the function $f$ and loss $L$ respectively:\n",
    "\n",
    "![ ](one-minus-neg-log.png)\n",
    "\n",
    "As $f_{\\vec{w},b}(\\vec{x}^{(i)}) -> 0$ then $loss -> 0$  \n",
    "As $f_{\\vec{w},b}(\\vec{x}^{(i)}) -> 1$ then $loss -> inf$\n",
    "\n",
    "--- \n",
    "\n",
    "Concluding that the further prediction $f_{\\vec{w},b}(\\vec{x}^{(i)})$ is from the target $y^{(i)}$, the higher the loss $L$. Plotting the nicer costs after the piecewise loss function is applied:\n",
    "\n",
    "![ ](good-cost.png)\n",
    "\n",
    "Why did we choose this function? Statistics! Specifically, the maximum likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simlifying the Cost Function\n",
    "\n",
    "Done for GD.\n",
    "\n",
    "Writing out the complete loss equation $L$ (equivalent to the piecewise form):\n",
    "\n",
    "$L(f_{\\vec{w},b}(\\vec{x}^{(i)}), y^{(i)}) = y^{(i)}log(f_{\\vec{w},b}(\\vec{x}^{(i)}) - (1 - y^{(i)})log(1 - f_{\\vec{w},b}(\\vec{x}^{(i)}), y^{(i)})$\n",
    "\n",
    "Using the cost:\n",
    "\n",
    "$J(w, \\vec{b}) = \\frac{1}{m}\\sum_{i=1}^m(L(f_{\\vec{w}, b}(x^{(i)}) - y^{(i)}))$\n",
    "\n",
    "Plugging in $L$ we get the general equation used for logistic regresion by GD:\n",
    "\n",
    "$J(w, \\vec{b}) = -\\frac{1}{m}\\sum_{i=1}^m[y^{(i)}log(f_{\\vec{w},b}(\\vec{x}^{(i)}) + (1 - y^{(i)})log(1 - f_{\\vec{w},b}(\\vec{x}^{(i)})]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ipympl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m get_ipython()\u001b[39m.\u001b[39;49mrun_line_magic(\u001b[39m'\u001b[39;49m\u001b[39mmatplotlib\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mwidget\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlab_utils_common\u001b[39;00m \u001b[39mimport\u001b[39;00m  plot_data, sigmoid, dlc\n",
      "File \u001b[1;32mb:\\COURSERA\\machine-learning\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2432\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2430\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mlocal_ns\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_local_scope(stack_depth)\n\u001b[0;32m   2431\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuiltin_trap:\n\u001b[1;32m-> 2432\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   2434\u001b[0m \u001b[39m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[0;32m   2435\u001b[0m \u001b[39m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[0;32m   2436\u001b[0m \u001b[39m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[0;32m   2437\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(fn, magic\u001b[39m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[39mFalse\u001b[39;00m):\n",
      "File \u001b[1;32mb:\\COURSERA\\machine-learning\\.venv\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:99\u001b[0m, in \u001b[0;36mPylabMagics.matplotlib\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAvailable matplotlib backends: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m backends_list)\n\u001b[0;32m     98\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 99\u001b[0m     gui, backend \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshell\u001b[39m.\u001b[39;49menable_matplotlib(args\u001b[39m.\u001b[39;49mgui\u001b[39m.\u001b[39;49mlower() \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(args\u001b[39m.\u001b[39;49mgui, \u001b[39mstr\u001b[39;49m) \u001b[39melse\u001b[39;49;00m args\u001b[39m.\u001b[39;49mgui)\n\u001b[0;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_show_matplotlib_backend(args\u001b[39m.\u001b[39mgui, backend)\n",
      "File \u001b[1;32mb:\\COURSERA\\machine-learning\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3621\u001b[0m, in \u001b[0;36mInteractiveShell.enable_matplotlib\u001b[1;34m(self, gui)\u001b[0m\n\u001b[0;32m   3617\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mWarning: Cannot change to a different GUI toolkit: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   3618\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m Using \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (gui, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpylab_gui_select))\n\u001b[0;32m   3619\u001b[0m         gui, backend \u001b[39m=\u001b[39m pt\u001b[39m.\u001b[39mfind_gui_and_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpylab_gui_select)\n\u001b[1;32m-> 3621\u001b[0m pt\u001b[39m.\u001b[39;49mactivate_matplotlib(backend)\n\u001b[0;32m   3622\u001b[0m configure_inline_support(\u001b[39mself\u001b[39m, backend)\n\u001b[0;32m   3624\u001b[0m \u001b[39m# Now we must activate the gui pylab wants to use, and fix %run to take\u001b[39;00m\n\u001b[0;32m   3625\u001b[0m \u001b[39m# plot updates into account\u001b[39;00m\n",
      "File \u001b[1;32mb:\\COURSERA\\machine-learning\\.venv\\lib\\site-packages\\IPython\\core\\pylabtools.py:368\u001b[0m, in \u001b[0;36mactivate_matplotlib\u001b[1;34m(backend)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[39m# Due to circular imports, pyplot may be only partially initialised\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[39m# when this function runs.\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[39m# So avoid needing matplotlib attribute-lookup to access pyplot.\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m pyplot \u001b[39mas\u001b[39;00m plt\n\u001b[1;32m--> 368\u001b[0m plt\u001b[39m.\u001b[39;49mswitch_backend(backend)\n\u001b[0;32m    370\u001b[0m plt\u001b[39m.\u001b[39mshow\u001b[39m.\u001b[39m_needmain \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[39m# We need to detect at runtime whether show() is called by the user.\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[39m# For this, we wrap it into a decorator which adds a 'called' flag.\u001b[39;00m\n",
      "File \u001b[1;32mb:\\COURSERA\\machine-learning\\.venv\\lib\\site-packages\\matplotlib\\pyplot.py:342\u001b[0m, in \u001b[0;36mswitch_backend\u001b[1;34m(newbackend)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[39m# have to escape the switch on access logic\u001b[39;00m\n\u001b[0;32m    340\u001b[0m old_backend \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__getitem__\u001b[39m(rcParams, \u001b[39m'\u001b[39m\u001b[39mbackend\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 342\u001b[0m module \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(cbook\u001b[39m.\u001b[39;49m_backend_module_name(newbackend))\n\u001b[0;32m    343\u001b[0m canvas_class \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39mFigureCanvas\n\u001b[0;32m    345\u001b[0m required_framework \u001b[39m=\u001b[39m canvas_class\u001b[39m.\u001b[39mrequired_interactive_framework\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:972\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:984\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ipympl'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from lab_utils_common import  plot_data, sigmoid, dlc\n",
    "plt.style.use('./deeplearning.mplstyle')\n",
    "\n",
    "X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])  #(m,n)\n",
    "y_train = np.array([0, 0, 0, 1, 1, 1])                                           #(m,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_logistic(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes cost\n",
    "\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]\n",
    "    cost = 0.0\n",
    "    for i in range(m):\n",
    "        z_i = np.dot(X[i],w) + b\n",
    "        f_wb_i = sigmoid(z_i)\n",
    "        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)\n",
    "             \n",
    "    cost = cost / m\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tmp = np.array([1,1])\n",
    "b_tmp = -3\n",
    "print(compute_cost_logistic(X_train, y_train, w_tmp, b_tmp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
