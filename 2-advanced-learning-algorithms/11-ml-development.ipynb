{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step by Step ML Development\n",
    "\n",
    "1. scope project & get data (\"honeypot projects\") with known outputs\n",
    "2. generate/distinguish features\n",
    "   - count specific patterns (boolean or other)\n",
    "   - other forms of existing features\n",
    "3. perform some error analysis and ensure that the features you are accounting for a majority\n",
    "4. train the model\n",
    "   - iterate back to data collection until the scope is clear\n",
    "5. add more data\n",
    "   - focus on areas error analysis indicated (this can boost performance by a lot)\n",
    "   - add labels to existing data first\n",
    "   - augment existing data (artificially)\n",
    "     - distort image: rotate, scale, transparency, mirroring, warpings\n",
    "     - add noise to sound: (crowds, bus, decrease quality, etc.)\n",
    "     - DO NOT AUGMENT TOWARDS MEANINGLESS DISTORTIONS!\n",
    "   - synthetic data (completely artificial)\n",
    "     - more common for visual tasks for computer vision\n",
    "   - transfer learning!\n",
    "\n",
    "Data-centric approach can be an effective way at improving the model without having to improve the model\n",
    "\n",
    "## Transfer Learning\n",
    "\n",
    "If it becomes difficult to add more data, you can actually transfer an existing nn (\"supervised-pretraining\") to your task for further tuning using the existing data you have. Note that you're constrained to inputs that have the same type and \"approach\". By only modifying the output layer, you're left with two options:\n",
    "\n",
    "1. only train the output params\n",
    "2. train all params\n",
    "\n",
    "![ ](transfer-learning.png)\n",
    "\n",
    "*Why does this work?*\n",
    "\n",
    "NNs don't typically classify the output context until the output layer, so you can harness the hardwork of a pre-trained model that is quite good at recognizing preliminary items. For ex, the first layer of a pre-trained image-recognition model is quite good at detecting edges, second corners, third curves & shapes.\n",
    "\n",
    "GPT3 is actually a result of transfer learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "You can take your ML model and place it on a server that is called to via an API (inference server). This way, you can target specific & better hardware that doesn't exist on the application functionality. \n",
    "\n",
    "MLOps: they maintain, monitor, update, etc. machine learning models ciritical to application functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics\n",
    "\n",
    "ML models are directly representative of the data they've been trained on, therefore engineers are liable for ensuring all training data is free from bias, wherever possible.\n",
    "\n",
    "### Guidelines\n",
    "\n",
    "- get a diverse team to brainstorm potential harms, with an emphasis on minorities and vulnerable groups\n",
    "- lit review on standards in the specific industry\n",
    "- audit the system against these possible harms\n",
    "- develop a mitigation plan/rollback system post deployment\n",
    "  - older version tags, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
